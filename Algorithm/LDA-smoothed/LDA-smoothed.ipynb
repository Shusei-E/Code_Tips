{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://satomacoto.blogspot.jp/2009/12/pythonlda.html   \n",
    "Code in the reference is modified for Python 3  \n",
    "\n",
    "LDA ([Blei et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Figure 1)  \n",
    "Variational Bayes EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,getopt\n",
    "from numpy import array,matrix,diag\n",
    "from scipy import sum,log,exp,mean,dot,ones,zeros\n",
    "from scipy.special import polygamma, gamma\n",
    "from scipy.linalg import norm\n",
    "from random import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Shusei/Dropbox/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(emmax = 100, beta_estimate = 1):\n",
    "    # set parameters\n",
    "    k = 10 # of classes to assume\n",
    "    #emmax = 2 # of maximum VB-EM iteration (default 100)\n",
    "    demmax = 20 # of maximum VB-EM iteration for a document\n",
    "    epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "    \n",
    "    # Train\n",
    "    train = open(\"train.txt\",'r').read()\n",
    "    alpha,phi, beta = ldamain(train, k, beta_estimate, emmax, demmax, epsilon)\n",
    "    \n",
    "    # Write\n",
    "    writer = open('output-alpha.txt','w')\n",
    "    writer.write(str(alpha.tolist()))\n",
    "    writer.close() \n",
    "    \n",
    "    writer = open('output-phi.txt','w')\n",
    "    writer.write(str(phi.tolist()))\n",
    "    writer.close()\n",
    "    \n",
    "    return alpha, phi, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ldamain(train, k, beta_estimate, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "    d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "    \n",
    "    data = []\n",
    "    for L in train.split(\"\\n\"):\n",
    "        if L == \"\":\n",
    "            continue\n",
    "\n",
    "        id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "        w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "        data.append([id_, w_count])\n",
    "    \n",
    "    return lda.train(data,k,beta_estimate,emmax,demmax, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lda():\n",
    "    '''\n",
    "    Latent Dirichlet Allocation, standard model.\n",
    "    [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "    d      : data of documents\n",
    "    k      : # of classes to assume\n",
    "    emmax  : # of maximum VB-EM iteration (default 100)\n",
    "    demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(d, k, beta_estimate, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation, standard model.\n",
    "        [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "        d      : data of documents\n",
    "        k      : # of classes to assume\n",
    "        emmax  : # of maximum VB-EM iteration (default 100)\n",
    "        demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "        '''\n",
    "        \n",
    "        # # of documents\n",
    "        M = len(d)\n",
    "        # # of words\n",
    "        L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "        \n",
    "        # initialize\n",
    "        beta = matrix(np.full((L, 1), 100, dtype=float) / L) # k x 1 matrix\n",
    "        phi = matrix(ones((k, L)) / L)\n",
    "        alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "        gammas = matrix(zeros((M, k)))\n",
    "        lik = 0\n",
    "        plik = lik\n",
    "        n_kv = matrix(np.random.rand(k, L))\n",
    "        phis = matrix(zeros((k, L)))\n",
    "        \n",
    "        #print ('number of documents (M)      = {0}'.format(M))\n",
    "        #print ('number of words (l)          = {0}'.format(L))\n",
    "        #print ('number of latent classes (k) = {0}'.format(k))\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            print ('iteration {0}/{1}..\\t'.format(j+1, emmax))\n",
    "            #vb-esstep\n",
    "            \n",
    "            nt_keep = []\n",
    "            for i in range(M):\n",
    "                gamma, q, nt, n_kv, xi_kv = lda.vbem(d[i], phi, alpha, beta, n_kv, demmax)\n",
    "                nt_keep.append(nt)\n",
    "                gammas[i,:] = gamma.T\n",
    "                phis = lda.accum_phi(phis,q,d[i], xi_kv)\n",
    "            #vb-mstep\n",
    "            alpha = lda.fpi_alpha(alpha, nt_keep)\n",
    "                    #alpha = lda.newton_alpha(gammas)\n",
    "            if beta_estimate == 1:\n",
    "                beta = lda.fpi_beta1(n_kv, beta)\n",
    "            else:\n",
    "                beta = lda.fpi_beta2(n_kv, beta)\n",
    "            phi = lda.mnormalize(phis,1)\n",
    "            #converge?\n",
    "#             lik = lda.lda_lik(d, phi, gammas)\n",
    "#             print ('log-likelihoood =', lik)\n",
    "#             if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "#                 if j < 5:\n",
    "#                     return lda.train(d, k, emmax, demmax) # try again\n",
    "#                 print ('converged')\n",
    "#                 return alpha, phi\n",
    "#             plik = lik\n",
    "            \n",
    "        return alpha, phi, beta\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def vbem(di, phi, alpha0, beta, n_kv, emmax=20):\n",
    "        '''\n",
    "        calculates a document and words posterior for a document d.\n",
    "        alpha  : Dirichlet posterior for a document d\n",
    "        q      : (Nd * K) matrix of word posterior over latent classes\n",
    "        di      : document data / here, only one sentence\n",
    "        phi   : \n",
    "        alpha0 : Dirichlet prior of alpha\n",
    "        emmax  : maximum # of VB-EM iteration.\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        Nd = len(di[0])\n",
    "        k = len(alpha0)\n",
    "        q = zeros((Nd, k))\n",
    "        nt = matrix(ones((1, k)) * Nd / k).T # initialize n_dk\n",
    "        pnt = nt\n",
    "        # xi_kv = n_kv + beta.T\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            #vb-estep\n",
    "            q = matrix( matrix(exp(digamma((n_kv + beta.T)[:,di[0]])).T) * diag(exp(digamma(alpha0 + nt))[:,0]))\n",
    "            q = lda.mnormalize(q , 1)\n",
    "                    # q(z_d,i =k)\n",
    "                    # alpha0 + ntでよくわからないけど、alphaをアップデートしてるからe-step?\n",
    "                    # better to look at original C code\n",
    "                       #  ap[k]というのが、式のexpの分子部分相当\n",
    "                        #  それにphiをかけて最後に正規化している\n",
    "            #vb-mstep\n",
    "            nt =  q.T * matrix(di[1]).T\n",
    "                    # nt:  probably expectation part in Sato Equation(3.89), same as bottom in Sato p.75\n",
    "                    #  Sato p.75にあるように本来ならば文章中の一語ずつチェックするが、データセットには\n",
    "                    #  各単語ごとの出現回数しかないので、単語が登場するたびに足すのではなく、単語の出現回数×qをして\n",
    "                    #  足しあわせている\n",
    "                    # better to look at original C code\n",
    "            for k_index in range(k): \n",
    "                n_kv[k_index, di[0]]  = q.T[k_index, :]\n",
    "                # Sato p.77下の更新式だけど、ここでは全てのdに対しては更新しないで、vbem()に流れてきているものだけを更新 <-- ！！！！！！この方法で良いのか不明！！！！！\n",
    "\n",
    "            #converge?\n",
    "            if j > 1 and np.absolute((nt - pnt).sum()) / k < 3 :\n",
    "                break\n",
    "            pnt = nt.copy()\n",
    "            pnkv = n_kv.copy()\n",
    "\n",
    "        alpha = alpha0 + nt # corresponds to Sato Eq (3.89) / dの全てのkに対して一度にしている\n",
    "                                        # nt:  probably expectation part in Sato Equation(3.89)\n",
    "        xi_kv = n_kv + beta.T  # Sato (3.95)\n",
    "        return alpha, q, nt, n_kv, xi_kv\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_alpha(alpha, nt_keep):\n",
    "        # fixed point iteration for alpha / Sato p.112\n",
    "        K = alpha.shape[0]\n",
    "        M = len(nt_keep)\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for k in range(K):\n",
    "            alpha_k = alpha[k, 0]\n",
    "\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for m in range(M):\n",
    "                nt = nt_keep[m]\n",
    "                nd = nt.sum() # Sato p.114\n",
    "\n",
    "                # numerator\n",
    "                numerator += (digamma(nt[k,0] + alpha_k) - digamma(alpha_k)) * alpha_k\n",
    "\n",
    "                # denominator\n",
    "                denominator += digamma(nd + alpha.sum()) - digamma(alpha.sum())\n",
    "\n",
    "            alpha[k,0] = numerator / denominator\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_beta1(n_kv, beta):\n",
    "        K, V = n_kv.shape\n",
    "        new_beta = matrix(zeros((V, 1)))\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for v in range(V):\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for k in range(K):\n",
    "                numerator += (digamma(n_kv[k, v] + beta[v, 0]) - digamma(beta[v, 0])) * beta[v, 0]\n",
    "                denominator += (digamma(n_kv[k, :].sum() + beta[v,0]) - digamma(beta.sum()))\n",
    "\n",
    "            new_beta[v, :]  = numerator / denominator\n",
    "            \n",
    "        return lda.normalize(new_beta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_beta2(n_kv, beta): # Sato (3.194)\n",
    "        K, V = n_kv.shape\n",
    "        new_beta = matrix(zeros((V, 1)))\n",
    "        digamma = lambda x: polygamma(0,x) \n",
    "\n",
    "        component1 = 0\n",
    "        for k in range(K):\n",
    "            for v in range(V):\n",
    "                component1 += n_kv[k,v]\n",
    "\n",
    "        numerator = 0\n",
    "        for v in range(V):\n",
    "            for k in range(K):\n",
    "                numerator += (digamma(n_kv[k,v] + beta[0,0]) - digamma(beta[0,0])) * beta[0,0]\n",
    "\n",
    "        denominator = 0\n",
    "        for k in range(K):\n",
    "            denominator += digamma(component1 + beta[0,0]) - digamma(V * beta[0,0])\n",
    "\n",
    "        new_beta.fill(numerator / denominator / V)\n",
    "        return lda.normalize(new_beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def accum_phi(phis, q, di, xi_kv):\n",
    "        '''\n",
    "        phis = accum_phi(phis,q,t)\n",
    "        accumulates word posteriors to latent classes.\n",
    "        phis : (V * K) matrix of summand\n",
    "        q     : (L * K) matrix of word posteriors\n",
    "        t     : document of struct array\n",
    "        '''\n",
    "        new_phis  = matrix(zeros((phis.shape[0], phis.shape[1])))\n",
    "        #for k in range(phis.shape[0]):\n",
    "        for v in range(phis.shape[1]):\n",
    "            new_phis[:, v] = matrix(np.random.dirichlet(np.squeeze(np.asarray(xi_kv[:,v])) , 1)).T\n",
    "\n",
    "            #new_phis[k, :] =  lda.normalize(new_phis[k, :])\n",
    "        \n",
    "        return new_phis\n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_lik(d, phi, gammas):\n",
    "        '''\n",
    "        lik = lda_lik(d, phi, gammas)\n",
    "        returns the likelihood of d, given LDA model of (phi, gammas).\n",
    "        '''\n",
    "        egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "        lik = 0\n",
    "        M = len(d)\n",
    "        for i in range(M):\n",
    "            t = d[i]\n",
    "            lik += (matrix(t[1]) * log(matrix(phi[:,t[0]]).T * egamma[i,:].T))[0,0]\n",
    "        return lik\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(v):\n",
    "        return v / sum(v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mnormalize(m, d=0):\n",
    "        '''\n",
    "        x = mnormalize(m, d)\n",
    "        normalizes a 2-D matrix m along the dimension d.\n",
    "        m : matrix\n",
    "        d : dimension to normalize (default 0)\n",
    "        '''\n",
    "        m = array(m)\n",
    "        v = sum(m, d)\n",
    "        if d == 0:\n",
    "            return m * matrix(diag(1.0 / v))\n",
    "        else:\n",
    "            return matrix(diag(1.0 / v)) * m\n",
    "        \n",
    "    @staticmethod\n",
    "    def converged(u, udash, threshold=1.0e-3):\n",
    "        '''\n",
    "        converged(u,udash,threshold)\n",
    "        Returns 1 if u and udash are not different by the ratio threshold\n",
    "        '''\n",
    "        return norm(u - udash) / norm(u) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/30..\t\n",
      "iteration 2/30..\t\n",
      "iteration 3/30..\t\n",
      "iteration 4/30..\t\n",
      "iteration 5/30..\t\n",
      "iteration 6/30..\t\n",
      "iteration 7/30..\t\n",
      "iteration 8/30..\t\n",
      "iteration 9/30..\t\n",
      "iteration 10/30..\t\n",
      "iteration 11/30..\t\n",
      "iteration 12/30..\t\n",
      "iteration 13/30..\t\n",
      "iteration 14/30..\t\n",
      "iteration 15/30..\t\n",
      "iteration 16/30..\t\n",
      "iteration 17/30..\t\n",
      "iteration 18/30..\t\n",
      "iteration 19/30..\t\n",
      "iteration 20/30..\t\n",
      "iteration 21/30..\t\n",
      "iteration 22/30..\t\n",
      "iteration 23/30..\t\n",
      "iteration 24/30..\t\n",
      "iteration 25/30..\t\n",
      "iteration 26/30..\t\n",
      "iteration 27/30..\t\n",
      "iteration 28/30..\t\n",
      "iteration 29/30..\t\n",
      "iteration 30/30..\t\n",
      "Topic:  0\n",
      "832     0.017136\n",
      "839     0.017136\n",
      "462     0.017136\n",
      "1119    0.017136\n",
      "1063    0.017136\n",
      "220     0.017136\n",
      "303     0.017136\n",
      "573     0.017136\n",
      "983     0.017136\n",
      "135     0.017136\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  1\n",
      "663     0.004365\n",
      "649     0.004365\n",
      "1123    0.004365\n",
      "1207    0.004365\n",
      "927     0.004365\n",
      "620     0.004365\n",
      "1203    0.004365\n",
      "623     0.004365\n",
      "1126    0.004365\n",
      "630     0.004365\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  2\n",
      "1065    0.003977\n",
      "541     0.003977\n",
      "531     0.003977\n",
      "953     0.003977\n",
      "954     0.003977\n",
      "533     0.003977\n",
      "957     0.003977\n",
      "304     0.003977\n",
      "1175    0.003977\n",
      "190     0.003977\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  3\n",
      "372     0.012679\n",
      "619     0.012679\n",
      "730     0.012679\n",
      "343     0.012679\n",
      "720     0.012679\n",
      "716     0.012679\n",
      "714     0.012679\n",
      "709     0.012679\n",
      "700     0.012679\n",
      "1169    0.012679\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  4\n",
      "686     0.040822\n",
      "101     0.040822\n",
      "963     0.040822\n",
      "120     0.040822\n",
      "116     0.040822\n",
      "564     0.040822\n",
      "1130    0.040822\n",
      "959     0.040822\n",
      "484     0.040822\n",
      "310     0.040822\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  5\n",
      "818     0.00202\n",
      "432     0.00202\n",
      "1050    0.00202\n",
      "428     0.00202\n",
      "427     0.00202\n",
      "421     0.00202\n",
      "420     0.00202\n",
      "413     0.00202\n",
      "872     0.00202\n",
      "408     0.00202\n",
      "Name: 5, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  6\n",
      "1223    0.018524\n",
      "1083    0.018524\n",
      "375     0.018524\n",
      "390     0.018524\n",
      "409     0.018524\n",
      "829     0.018524\n",
      "461     0.018524\n",
      "483     0.018524\n",
      "487     0.018524\n",
      "542     0.018524\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  7\n",
      "572     0.017556\n",
      "411     0.017556\n",
      "367     0.017556\n",
      "192     0.017556\n",
      "726     0.017556\n",
      "1229    0.017556\n",
      "728     0.017556\n",
      "1057    0.017556\n",
      "293     0.017556\n",
      "902     0.017556\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  8\n",
      "733     0.023748\n",
      "1187    0.023748\n",
      "59      0.023748\n",
      "1152    0.023748\n",
      "703     0.023748\n",
      "1167    0.023748\n",
      "1011    0.023748\n",
      "507     0.023748\n",
      "1293    0.023748\n",
      "723     0.023748\n",
      "Name: 8, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  9\n",
      "729     0.028871\n",
      "964     0.028871\n",
      "1306    0.028871\n",
      "196     0.028871\n",
      "761     0.028871\n",
      "602     0.028871\n",
      "594     0.028871\n",
      "1312    0.028871\n",
      "140     0.028871\n",
      "209     0.028871\n",
      "Name: 9, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha, phi, beta = main(emmax=30, beta_estimate = 1)\n",
    "    \n",
    "    res_b = pd.DataFrame(phi)\n",
    "    for i in range(phi.shape[0]):\n",
    "        res_temp = res_b.ix[i, :].sort_values(ascending=False)[:10]\n",
    "        res_temp.index += 1\n",
    "        print(\"Topic: \", i)\n",
    "        print(res_temp)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  2.18956815],\n",
       "        [  6.88474062],\n",
       "        [  8.10239142],\n",
       "        [  2.69850794],\n",
       "        [  0.97705284],\n",
       "        [ 15.53293102],\n",
       "        [  2.10718438],\n",
       "        [  2.59970003],\n",
       "        [  1.77833913],\n",
       "        [  1.46101818]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.00741916],\n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968],\n",
       "        ..., \n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
