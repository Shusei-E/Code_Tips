{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://satomacoto.blogspot.jp/2009/12/pythonlda.html   \n",
    "Code in the reference is modified for Python 3  \n",
    "\n",
    "LDA ([Blei et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Figure 1)  \n",
    "Variational Bayes EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,getopt\n",
    "from numpy import array,matrix,diag\n",
    "from scipy import sum,log,exp,mean,dot,ones,zeros\n",
    "from scipy.special import polygamma, gamma\n",
    "from scipy.linalg import norm\n",
    "from random import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/Shusei/Dropbox/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(emmax = 100, beta_estimate = 1):\n",
    "    # set parameters\n",
    "    k = 10 # of classes to assume\n",
    "    #emmax = 2 # of maximum VB-EM iteration (default 100)\n",
    "    demmax = 20 # of maximum VB-EM iteration for a document\n",
    "    epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "    \n",
    "    # Train\n",
    "    train = open(\"train.txt\",'r').read()\n",
    "    alpha,phi, beta = ldamain(train, k, beta_estimate, emmax, demmax, epsilon)\n",
    "    \n",
    "    # Write\n",
    "    writer = open('output-alpha.txt','w')\n",
    "    writer.write(str(alpha.tolist()))\n",
    "    writer.close() \n",
    "    \n",
    "    writer = open('output-phi.txt','w')\n",
    "    writer.write(str(phi.tolist()))\n",
    "    writer.close()\n",
    "    \n",
    "    return alpha, phi, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ldamain(train, k, beta_estimate, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "    d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "    \n",
    "    data = []\n",
    "    for L in train.split(\"\\n\"):\n",
    "        if L == \"\":\n",
    "            continue\n",
    "\n",
    "        id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "        w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "        data.append([id_, w_count])\n",
    "    \n",
    "    return lda.train(data,k,beta_estimate,emmax,demmax, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lda():\n",
    "    '''\n",
    "    Latent Dirichlet Allocation, standard model.\n",
    "    [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "    d      : data of documents\n",
    "    k      : # of classes to assume\n",
    "    emmax  : # of maximum VB-EM iteration (default 100)\n",
    "    demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(d, k, beta_estimate, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation, standard model.\n",
    "        [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "        d      : data of documents\n",
    "        k      : # of classes to assume\n",
    "        emmax  : # of maximum VB-EM iteration (default 100)\n",
    "        demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "        '''\n",
    "        \n",
    "        # # of documents\n",
    "        M = len(d)\n",
    "        # # of words\n",
    "        L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "        \n",
    "        # initialize\n",
    "        beta = matrix(np.full((L, 1), 100, dtype=float) / L) # k x 1 matrix\n",
    "        phi = matrix(ones((k, L)) / L)\n",
    "        alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "        gammas = matrix(zeros((M, k)))\n",
    "        lik = 0\n",
    "        plik = lik\n",
    "        n_kv = matrix(np.random.rand(k, L))\n",
    "        phis = matrix(zeros((k, L)))\n",
    "        \n",
    "        #print ('number of documents (M)      = {0}'.format(M))\n",
    "        #print ('number of words (l)          = {0}'.format(L))\n",
    "        #print ('number of latent classes (k) = {0}'.format(k))\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            print ('iteration {0}/{1}..\\t'.format(j+1, emmax))\n",
    "            #vb-esstep\n",
    "            \n",
    "            nt_keep = []\n",
    "            for i in range(M):\n",
    "                gamma, q, nt, n_kv, xi_kv = lda.vbem(d[i], phi, alpha, beta, n_kv, demmax)\n",
    "                nt_keep.append(nt)\n",
    "                gammas[i,:] = gamma.T\n",
    "                phis = lda.accum_phi(phis,q,d[i], xi_kv)\n",
    "            #vb-mstep\n",
    "            alpha = lda.fpi_alpha(alpha, nt_keep)\n",
    "                    #alpha = lda.newton_alpha(gammas)\n",
    "            if beta_estimate == 1:\n",
    "                beta = lda.fpi_beta1(n_kv, beta)\n",
    "            else:\n",
    "                beta = lda.fpi_beta2(n_kv, beta)\n",
    "            phi = lda.mnormalize(phis,1)\n",
    "            #converge?\n",
    "#             lik = lda.lda_lik(d, phi, gammas)\n",
    "#             print ('log-likelihoood =', lik)\n",
    "#             if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "#                 if j < 5:\n",
    "#                     return lda.train(d, k, emmax, demmax) # try again\n",
    "#                 print ('converged')\n",
    "#                 return alpha, phi\n",
    "#             plik = lik\n",
    "            \n",
    "        return alpha, phi, beta\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def vbem(di, phi, alpha0, beta, n_kv, emmax=20):\n",
    "        '''\n",
    "        calculates a document and words posterior for a document d.\n",
    "        alpha  : Dirichlet posterior for a document d\n",
    "        q      : (Nd * K) matrix of word posterior over latent classes\n",
    "        di      : document data / here, only one sentence\n",
    "        phi   : \n",
    "        alpha0 : Dirichlet prior of alpha\n",
    "        emmax  : maximum # of VB-EM iteration.\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        Nd = len(di[0])\n",
    "        k = len(alpha0)\n",
    "        q = zeros((Nd, k))\n",
    "        nt = matrix(ones((1, k)) * Nd / k).T # initialize n_dk\n",
    "        pnt = nt\n",
    "        # xi_kv = n_kv + beta.T\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            #vb-estep\n",
    "            q = matrix( matrix(exp(digamma((n_kv + beta.T)[:,di[0]])).T) * diag(exp(digamma(alpha0 + nt))[:,0]))\n",
    "            q = lda.mnormalize(q , 1)\n",
    "                    # q(z_d,i =k)\n",
    "                    # alpha0 + ntでよくわからないけど、alphaをアップデートしてるからe-step?\n",
    "                    # better to look at original C code\n",
    "                       #  ap[k]というのが、式のexpの分子部分相当\n",
    "                        #  それにphiをかけて最後に正規化している\n",
    "            #vb-mstep\n",
    "            nt =  q.T * matrix(di[1]).T\n",
    "                    # nt:  probably expectation part in Sato Equation(3.89), same as bottom in Sato p.75\n",
    "                    #  Sato p.75にあるように本来ならば文章中の一語ずつチェックするが、データセットには\n",
    "                    #  各単語ごとの出現回数しかないので、単語が登場するたびに足すのではなく、単語の出現回数×qをして\n",
    "                    #  足しあわせている\n",
    "                    # better to look at original C code\n",
    "            for k_index in range(k): \n",
    "                n_kv[k_index, di[0]]  = q.T[k_index, :]\n",
    "                # Sato p.77下の更新式だけど、ここでは全てのdに対しては更新しないで、vbem()に流れてきているものだけを更新 <-- ！！！！！！この方法で良いのか不明！！！！！\n",
    "\n",
    "            #converge?\n",
    "            if j > 1 and np.absolute((nt - pnt).sum()) / k < 3 :\n",
    "                break\n",
    "            pnt = nt.copy()\n",
    "            pnkv = n_kv.copy()\n",
    "\n",
    "        alpha = alpha0 + nt # corresponds to Sato Eq (3.89) / dの全てのkに対して一度にしている\n",
    "                                        # nt:  probably expectation part in Sato Equation(3.89)\n",
    "        xi_kv = n_kv + beta.T  # Sato (3.95)\n",
    "        return alpha, q, nt, n_kv, xi_kv\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_alpha(alpha, nt_keep):\n",
    "        # fixed point iteration for alpha / Sato p.112\n",
    "        K = alpha.shape[0]\n",
    "        M = len(nt_keep)\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for k in range(K):\n",
    "            alpha_k = alpha[k, 0]\n",
    "\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for m in range(M):\n",
    "                nt = nt_keep[m]\n",
    "                nd = nt.sum() # Sato p.114\n",
    "\n",
    "                # numerator\n",
    "                numerator += (digamma(nt[k,0] + alpha_k) - digamma(alpha_k)) * alpha_k\n",
    "\n",
    "                # denominator\n",
    "                denominator += digamma(nd + alpha.sum()) - digamma(alpha.sum())\n",
    "\n",
    "            alpha[k,0] = numerator / denominator\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_beta1(n_kv, beta):\n",
    "        K, V = n_kv.shape\n",
    "        new_beta = matrix(zeros((V, 1)))\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for v in range(V):\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for k in range(K):\n",
    "                numerator += (digamma(n_kv[k, v] + beta[v, 0]) - digamma(beta[v, 0])) * beta[v, 0]\n",
    "                denominator += (digamma(n_kv[k, :].sum() + beta[v,0]) - digamma(beta.sum()))\n",
    "\n",
    "            new_beta[v, :]  = numerator / denominator\n",
    "            \n",
    "        return lda.normalize(new_beta)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_beta2(n_kv, beta): # Sato (3.194)\n",
    "        K, V = n_kv.shape\n",
    "        new_beta = matrix(zeros((V, 1)))\n",
    "        digamma = lambda x: polygamma(0,x) \n",
    "\n",
    "        component1 = 0\n",
    "        for k in range(K):\n",
    "            for v in range(V):\n",
    "                component1 += n_kv[k,v]\n",
    "\n",
    "        numerator = 0\n",
    "        for v in range(V):\n",
    "            for k in range(K):\n",
    "                numerator += (digamma(n_kv[k,v] + beta[0,0]) - digamma(beta[0,0])) * beta[0,0]\n",
    "\n",
    "        denominator = 0\n",
    "        for k in range(K):\n",
    "            denominator += digamma(component1 + beta[0,0]) - digamma(V * beta[0,0])\n",
    "\n",
    "        new_beta.fill(numerator / denominator / V)\n",
    "        return lda.normalize(new_beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def accum_phi(phis, q, di, xi_kv):\n",
    "        '''\n",
    "        phis = accum_phi(phis,q,t)\n",
    "        accumulates word posteriors to latent classes.\n",
    "        phis : (V * K) matrix of summand\n",
    "        q     : (L * K) matrix of word posteriors\n",
    "        t     : document of struct array\n",
    "        '''\n",
    "        new_phis  = matrix(zeros((phis.shape[0], phis.shape[1])))\n",
    "        #for k in range(phis.shape[0]):\n",
    "        for k in range(phis.shape[0]):\n",
    "            new_phis[k, :] = matrix(np.random.dirichlet(np.squeeze(np.asarray(xi_kv[k,:])) , 1))\n",
    "\n",
    "            #new_phis[k, :] =  lda.normalize(new_phis[k, :])\n",
    "        \n",
    "        return new_phis\n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_lik(d, phi, gammas):\n",
    "        '''\n",
    "        lik = lda_lik(d, phi, gammas)\n",
    "        returns the likelihood of d, given LDA model of (phi, gammas).\n",
    "        '''\n",
    "        egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "        lik = 0\n",
    "        M = len(d)\n",
    "        for i in range(M):\n",
    "            t = d[i]\n",
    "            lik += (matrix(t[1]) * log(matrix(phi[:,t[0]]).T * egamma[i,:].T))[0,0]\n",
    "        return lik\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(v):\n",
    "        return v / sum(v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mnormalize(m, d=0):\n",
    "        '''\n",
    "        x = mnormalize(m, d)\n",
    "        normalizes a 2-D matrix m along the dimension d.\n",
    "        m : matrix\n",
    "        d : dimension to normalize (default 0)\n",
    "        '''\n",
    "        m = array(m)\n",
    "        v = sum(m, d)\n",
    "        if d == 0:\n",
    "            return m * matrix(diag(1.0 / v))\n",
    "        else:\n",
    "            return matrix(diag(1.0 / v)) * m\n",
    "        \n",
    "    @staticmethod\n",
    "    def converged(u, udash, threshold=1.0e-3):\n",
    "        '''\n",
    "        converged(u,udash,threshold)\n",
    "        Returns 1 if u and udash are not different by the ratio threshold\n",
    "        '''\n",
    "        return norm(u - udash) / norm(u) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/3..\t\n",
      "iteration 2/3..\t\n",
      "iteration 3/3..\t\n",
      "Topic:  0\n",
      "1100    0.071711\n",
      "572     0.065037\n",
      "1086    0.055851\n",
      "727     0.054736\n",
      "369     0.050399\n",
      "1275    0.046809\n",
      "1286    0.044045\n",
      "749     0.042101\n",
      "71      0.039751\n",
      "565     0.035193\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  1\n",
      "1285    0.015990\n",
      "93      0.015586\n",
      "291     0.015126\n",
      "890     0.014509\n",
      "202     0.012072\n",
      "25      0.011778\n",
      "113     0.010847\n",
      "736     0.010277\n",
      "1310    0.009868\n",
      "816     0.009846\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  2\n",
      "613     0.332332\n",
      "497     0.159185\n",
      "768     0.148500\n",
      "550     0.084286\n",
      "875     0.082851\n",
      "985     0.059179\n",
      "1087    0.038765\n",
      "223     0.025286\n",
      "44      0.021841\n",
      "1324    0.017517\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  3\n",
      "299     0.047470\n",
      "922     0.040600\n",
      "232     0.039056\n",
      "1179    0.028737\n",
      "1276    0.027522\n",
      "1020    0.027256\n",
      "557     0.025591\n",
      "1223    0.021993\n",
      "686     0.021670\n",
      "1076    0.020905\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  4\n",
      "1012    0.061395\n",
      "328     0.044986\n",
      "972     0.043639\n",
      "298     0.042540\n",
      "186     0.035856\n",
      "654     0.033614\n",
      "1212    0.030701\n",
      "773     0.030229\n",
      "884     0.030048\n",
      "139     0.025583\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  5\n",
      "747     0.155467\n",
      "638     0.139391\n",
      "553     0.101213\n",
      "741     0.087937\n",
      "200     0.075617\n",
      "948     0.052193\n",
      "1071    0.051330\n",
      "880     0.042807\n",
      "510     0.035708\n",
      "96      0.035110\n",
      "Name: 5, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  6\n",
      "700     0.174961\n",
      "1068    0.133712\n",
      "955     0.085606\n",
      "1045    0.071037\n",
      "1288    0.069413\n",
      "354     0.065928\n",
      "810     0.047590\n",
      "740     0.035583\n",
      "1220    0.031896\n",
      "857     0.031895\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  7\n",
      "1256    0.494081\n",
      "838     0.301797\n",
      "261     0.078862\n",
      "49      0.057685\n",
      "1       0.026260\n",
      "579     0.025750\n",
      "250     0.009474\n",
      "877     0.004689\n",
      "59      0.001309\n",
      "358     0.000030\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  8\n",
      "423     0.034691\n",
      "1031    0.029933\n",
      "1134    0.029020\n",
      "281     0.022889\n",
      "927     0.022124\n",
      "211     0.021880\n",
      "743     0.018523\n",
      "120     0.017470\n",
      "1028    0.016255\n",
      "882     0.016120\n",
      "Name: 8, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  9\n",
      "711    0.010840\n",
      "658    0.010591\n",
      "856    0.009375\n",
      "624    0.008693\n",
      "56     0.008658\n",
      "401    0.008571\n",
      "615    0.008456\n",
      "757    0.008447\n",
      "761    0.008373\n",
      "38     0.008368\n",
      "Name: 9, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha, phi, beta = main(emmax=3, beta_estimate = 1)\n",
    "    \n",
    "    res_b = pd.DataFrame(phi)\n",
    "    for i in range(phi.shape[0]):\n",
    "        res_temp = res_b.ix[i, :].sort_values(ascending=False)[:10]\n",
    "        res_temp.index += 1\n",
    "        print(\"Topic: \", i)\n",
    "        print(res_temp)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1325)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  2.18956815],\n",
       "        [  6.88474062],\n",
       "        [  8.10239142],\n",
       "        [  2.69850794],\n",
       "        [  0.97705284],\n",
       "        [ 15.53293102],\n",
       "        [  2.10718438],\n",
       "        [  2.59970003],\n",
       "        [  1.77833913],\n",
       "        [  1.46101818]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.00741916],\n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968],\n",
       "        ..., \n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968],\n",
       "        [ 0.00074968]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
