{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://satomacoto.blogspot.jp/2009/12/pythonlda.html   \n",
    "Code in the reference is modified for Python 3  \n",
    "\n",
    "LDA ([Blei et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Figure 1)  \n",
    "Variational Bayes EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,getopt\n",
    "from numpy import array,matrix,diag\n",
    "from scipy import sum,log,exp,mean,dot,ones,zeros\n",
    "from scipy.special import polygamma, gamma\n",
    "from scipy.linalg import norm\n",
    "from random import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/3928941380/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set parameters\n",
    "    k = 10 # of classes to assume\n",
    "    emmax = 100 # of maximum VB-EM iteration (default 100)\n",
    "    demmax = 20 # of maximum VB-EM iteration for a document\n",
    "    epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "    \n",
    "    # Train\n",
    "    train = open(\"train.txt\",'r').read()\n",
    "    alpha,phi = ldamain(train, k, emmax, demmax, epsilon)\n",
    "    \n",
    "    # Write\n",
    "    writer = open('output-alpha.txt','w')\n",
    "    writer.write(str(alpha.tolist()))\n",
    "    writer.close() \n",
    "    \n",
    "    writer = open('output-phi.txt','w')\n",
    "    writer.write(str(phi.tolist()))\n",
    "    writer.close()\n",
    "    \n",
    "    return alpha, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ldamain(train, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "    d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "    \n",
    "    data = []\n",
    "    for L in train.split(\"\\n\"):\n",
    "        if L == \"\":\n",
    "            continue\n",
    "\n",
    "        id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "        w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "        data.append([id_, w_count])\n",
    "    \n",
    "    return lda.train(data,k,emmax,demmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = open(\"train.txt\",'r').read()\n",
    "d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "\n",
    "data = []\n",
    "for L in train.split(\"\\n\"):\n",
    "    if L == \"\":\n",
    "        continue\n",
    "\n",
    "    id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "    w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "    data.append([id_, w_count])\n",
    "    \n",
    "d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10 # of classes to assume\n",
    "emmax = 100 # of maximum VB-EM iteration (default 100)\n",
    "demmax = 20 # of maximum VB-EM iteration for a document\n",
    "epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "\n",
    "# # of documents\n",
    "M = len(d)\n",
    "# # of words\n",
    "L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "\n",
    "# initialize\n",
    "beta = matrix(np.full((L, 1), 100, dtype=float) / L) # L x 1 matrix\n",
    "phi = matrix(ones((k, L)) / L)\n",
    "alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "gammas = matrix(zeros((M, k)))\n",
    "lik = 0\n",
    "plik = lik\n",
    "phis = matrix(zeros((k, L)))\n",
    "n_kv = matrix(np.random.rand(k, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihoood = nan\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-9-d3adfa2e7796>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-d3adfa2e7796>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    return lda.train(d, k, emmax, demmax) # try again\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "phis = matrix(zeros((k, L)))\n",
    "nt_keep = []\n",
    "for i in range(M):\n",
    "    gamma, q, nt, n_kv, xi_kv = lda.vbem(d[i], phi, alpha, beta, n_kv, demmax)\n",
    "    nt_keep.append(nt)\n",
    "    gammas[i,:] = gamma.T\n",
    "    phis = lda.accum_phi(phis,q,d[i], xi_kv)\n",
    "#vb-mstep\n",
    "alpha = lda.fpi_alpha(alpha, nt_keep)\n",
    "        #alpha = lda.newton_alpha(gammas)\n",
    "beta = lda.fpi_beta(n_kv, beta)\n",
    "phi = lda.mnormalize(phis,1)\n",
    "#converge?\n",
    "lik = lda.lda_lik(d, phi, gammas)\n",
    "print ('log-likelihoood =', lik)\n",
    "if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "    if j < 5:\n",
    "        print\n",
    "        return lda.train(d, k, emmax, demmax) # try again\n",
    "        return\n",
    "    print ('converged')\n",
    "    return alpha, phi\n",
    "plik = lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208.99999999999991"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lda():\n",
    "    '''\n",
    "    Latent Dirichlet Allocation, standard model.\n",
    "    [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "    d      : data of documents\n",
    "    k      : # of classes to assume\n",
    "    emmax  : # of maximum VB-EM iteration (default 100)\n",
    "    demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(d, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation, standard model.\n",
    "        [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "        d      : data of documents\n",
    "        k      : # of classes to assume\n",
    "        emmax  : # of maximum VB-EM iteration (default 100)\n",
    "        demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "        '''\n",
    "        \n",
    "        # # of documents\n",
    "        M = len(d)\n",
    "        # # of words\n",
    "        L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "        \n",
    "        # initialize\n",
    "        beta = matrix(np.full((L, 1), 100, dtype=float) / L) # k x 1 matrix\n",
    "        phi = matrix(ones((k, L)) / L)\n",
    "        alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "        gammas = matrix(zeros((M, k)))\n",
    "        lik = 0\n",
    "        plik = lik\n",
    "        n_kv = matrix(np.random.rand(k, L))\n",
    "        phis = matrix(zeros((k, L)))\n",
    "        \n",
    "        #print ('number of documents (M)      = {0}'.format(M))\n",
    "        #print ('number of words (l)          = {0}'.format(L))\n",
    "        #print ('number of latent classes (k) = {0}'.format(k))\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            print ('iteration {0}/{1}..\\t'.format(j+1, emmax))\n",
    "            #vb-esstep\n",
    "            \n",
    "            nt_keep = []\n",
    "            for i in range(M):\n",
    "                gamma, q, nt, n_kv, xi_kv = lda.vbem(d[i], phi, alpha, beta, n_kv, demmax)\n",
    "                nt_keep.append(nt)\n",
    "                gammas[i,:] = gamma.T\n",
    "                phis = lda.accum_phi(phis,q,d[i], xi_kv)\n",
    "            #vb-mstep\n",
    "            alpha = lda.fpi_alpha(alpha, nt_keep)\n",
    "                    #alpha = lda.newton_alpha(gammas)\n",
    "            beta = lda.fpi_beta(n_kv, beta)\n",
    "            phi = lda.mnormalize(phis,1)\n",
    "            #converge?\n",
    "#             lik = lda.lda_lik(d, phi, gammas)\n",
    "#             print ('log-likelihoood =', lik)\n",
    "#             if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "#                 if j < 5:\n",
    "#                     return lda.train(d, k, emmax, demmax) # try again\n",
    "#                 print ('converged')\n",
    "#                 return alpha, phi\n",
    "#             plik = lik\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def vbem(di, phi, alpha0, beta, n_kv, emmax=20):\n",
    "        '''\n",
    "        calculates a document and words posterior for a document d.\n",
    "        alpha  : Dirichlet posterior for a document d\n",
    "        q      : (Nd * K) matrix of word posterior over latent classes\n",
    "        di      : document data / here, only one sentence\n",
    "        phi   : \n",
    "        alpha0 : Dirichlet prior of alpha\n",
    "        emmax  : maximum # of VB-EM iteration.\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        Nd = len(di[0])\n",
    "        k = len(alpha0)\n",
    "        q = zeros((Nd, k))\n",
    "        nt = matrix(ones((1, k)) * Nd / k).T # initialize n_dk\n",
    "        pnt = nt\n",
    "        # xi_kv = n_kv + beta.T\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            #vb-estep\n",
    "            q = matrix( matrix(exp(digamma((n_kv + beta.T)[:,di[0]])).T) * diag(exp(digamma(alpha0 + nt))[:,0]))\n",
    "            q = lda.mnormalize(q , 1)\n",
    "                    # q(z_d,i =k)\n",
    "                    # alpha0 + ntでよくわからないけど、alphaをアップデートしてるからe-step?\n",
    "                    # better to look at original C code\n",
    "                       #  ap[k]というのが、式のexpの分子部分相当\n",
    "                        #  それにphiをかけて最後に正規化している\n",
    "            #vb-mstep\n",
    "            nt =  q.T * matrix(di[1]).T\n",
    "                    # nt:  probably expectation part in Sato Equation(3.89), same as bottom in Sato p.75\n",
    "                    #  Sato p.75にあるように本来ならば文章中の一語ずつチェックするが、データセットには\n",
    "                    #  各単語ごとの出現回数しかないので、単語が登場するたびに足すのではなく、単語の出現回数×qをして\n",
    "                    #  足しあわせている\n",
    "                    # better to look at original C code\n",
    "            for k_index in range(k): \n",
    "                n_kv[k_index, di[0]]  = q.T[k_index, :]\n",
    "                # Sato p.77下の更新式だけど、ここでは全てのdに対しては更新しないで、vbem()に流れてきているものだけを更新 <-- ！！！！！！この方法で良いのか不明！！！！！\n",
    "\n",
    "            #converge?\n",
    "            if j > 1 and lda.converged(nt, pnt, 1.0e-2) and lda.converged(n_kv, pnkv, 1.0e-2):\n",
    "                break\n",
    "            pnt = nt.copy()\n",
    "            pnkv = n_kv.copy()\n",
    "\n",
    "        alpha = alpha0 + nt # corresponds to Sato Eq (3.89) / dの全てのkに対して一度にしている\n",
    "                                        # nt:  probably expectation part in Sato Equation(3.89)\n",
    "        xi_kv = n_kv + beta.T  # Sato (3.89)\n",
    "        return alpha, q, nt, n_kv, xi_kv\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_alpha(alpha, nt_keep):\n",
    "        # fixed point iteration for alpha / Sato p.112\n",
    "        K = alpha.shape[0]\n",
    "        M = len(nt_keep)\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for k in range(K):\n",
    "            alpha_k = alpha[k, 0]\n",
    "\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for m in range(M):\n",
    "                nt = nt_keep[m]\n",
    "                nd = nt.sum() # Sato p.114\n",
    "\n",
    "                # numerator\n",
    "                numerator += (digamma(nt[k,0] + alpha_k) - digamma(alpha_k)) * alpha_k\n",
    "\n",
    "                # denominator\n",
    "                denominator += digamma(nd + alpha.sum()) - digamma(alpha.sum())\n",
    "\n",
    "            alpha[k,0] = numerator / denominator\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_beta(n_kv, beta):\n",
    "        K, V = n_kv.shape\n",
    "        new_beta = matrix(zeros((V, 1)))\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        component1 = 0\n",
    "        for v in range(V):\n",
    "            component1 += n_kv\n",
    "\n",
    "        for v1 in range(V):\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for k in range(K):\n",
    "                numerator += (digamma(n_kv[k, v] + beta[v, 0]) - digamma(beta[v, 0])) * beta[v, 0]\n",
    "                denominator += (digamma(n_kv[k, :].sum() + beta[v,0]) - digamma(beta.sum()))\n",
    "\n",
    "            new_beta[v, :]  = numerator / denominator\n",
    "            \n",
    "        return lda.normalize(new_beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def accum_phi(phis, q, di, xi_kv):\n",
    "        '''\n",
    "        phis = accum_phi(phis,q,t)\n",
    "        accumulates word posteriors to latent classes.\n",
    "        phis : (V * K) matrix of summand\n",
    "        q     : (L * K) matrix of word posteriors\n",
    "        t     : document of struct array\n",
    "        '''\n",
    "        new_phis  = matrix(zeros((phis.shape[0], phis.shape[1])))\n",
    "        for k in range(phis.shape[0]):\n",
    "            for v in range(phis.shape[1]):\n",
    "                new_phis[k, v] = np.power(phis[k,v], xi_kv[k,v])\n",
    "\n",
    "            new_phis[k, :] =  lda.normalize(new_phis[k, :])\n",
    "        \n",
    "        return new_phis\n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_lik(d, phi, gammas):\n",
    "        '''\n",
    "        lik = lda_lik(d, phi, gammas)\n",
    "        returns the likelihood of d, given LDA model of (phi, gammas).\n",
    "        '''\n",
    "        egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "        lik = 0\n",
    "        M = len(d)\n",
    "        for i in range(M):\n",
    "            t = d[i]\n",
    "            lik += (matrix(t[1]) * log(matrix(phi[:,t[0]]).T * egamma[i,:].T))[0,0]\n",
    "        return lik\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(v):\n",
    "        return v / sum(v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mnormalize(m, d=0):\n",
    "        '''\n",
    "        x = mnormalize(m, d)\n",
    "        normalizes a 2-D matrix m along the dimension d.\n",
    "        m : matrix\n",
    "        d : dimension to normalize (default 0)\n",
    "        '''\n",
    "        m = array(m)\n",
    "        v = sum(m, d)\n",
    "        if d == 0:\n",
    "            return m * matrix(diag(1.0 / v))\n",
    "        else:\n",
    "            return matrix(diag(1.0 / v)) * m\n",
    "        \n",
    "    @staticmethod\n",
    "    def converged(u, udash, threshold=1.0e-3):\n",
    "        '''\n",
    "        converged(u,udash,threshold)\n",
    "        Returns 1 if u and udash are not different by the ratio threshold\n",
    "        '''\n",
    "        return norm(u - udash) / norm(u) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1/100..\t\n",
      "log-likelihoood = nan\n",
      "iteration 2/100..\t\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-242bf6807d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mres_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9d332eb5f451>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldamain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Write\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ccaa5fe04fb6>\u001b[0m in \u001b[0;36mldamain\u001b[0;34m(train, k, emmax, demmax, epsilon)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdemmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-818295de76bc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d, k, emmax, demmax, epsilon)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mnt_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvbem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mnt_keep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mgammas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-818295de76bc>\u001b[0m in \u001b[0;36mvbem\u001b[0;34m(di, phi, alpha0, beta, n_kv, emmax)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m#converge?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0e-2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverged\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_kv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpnkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mpnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-818295de76bc>\u001b[0m in \u001b[0;36mconverged\u001b[0;34m(u, udash, threshold)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mudash\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mratio\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         '''\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mudash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/scipy/linalg/misc.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# Differs from numpy only in non-finite handling and the use of blas.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Only use optimized norms if axis and keepdims are not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         raise ValueError(\n\u001b[0;32m--> 668\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    669\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha, phi = main()\n",
    "    \n",
    "    res_b = pd.DataFrame(phi)\n",
    "    for i in range(phi.shape[0]):\n",
    "        res_temp = res_b.ix[i, :].sort_values(ascending=False)[:10]\n",
    "        res_temp.index += 1\n",
    "        print(\"Topic: \", i)\n",
    "        print(res_temp)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
