{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://satomacoto.blogspot.jp/2009/12/pythonlda.html   \n",
    "Code in the reference is modified for Python 3  \n",
    "\n",
    "LDA ([Blei et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Figure 1)  \n",
    "Variational Bayes EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,getopt\n",
    "from numpy import array,matrix,diag\n",
    "from scipy import sum,log,exp,mean,dot,ones,zeros\n",
    "from scipy.special import polygamma\n",
    "from scipy.linalg import norm\n",
    "from random import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/3928941380/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set parameters\n",
    "    k = 10 # of classes to assume\n",
    "    emmax = 100 # of maximum VB-EM iteration (default 100)\n",
    "    demmax = 20 # of maximum VB-EM iteration for a document\n",
    "    epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "    \n",
    "    # Train\n",
    "    train = open(\"train.txt\",'r').read()\n",
    "    alpha,phi = ldamain(train, k, emmax, demmax, epsilon)\n",
    "    \n",
    "    # Write\n",
    "    writer = open('output-alpha.txt','w')\n",
    "    writer.write(str(alpha.tolist()))\n",
    "    writer.close() \n",
    "    \n",
    "    writer = open('output-phi.txt','w')\n",
    "    writer.write(str(phi.tolist()))\n",
    "    writer.close()\n",
    "    \n",
    "    return alpha, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ldamain(train, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "    d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "    \n",
    "    data = []\n",
    "    for L in train.split(\"\\n\"):\n",
    "        if L == \"\":\n",
    "            continue\n",
    "\n",
    "        id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "        w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "        data.append([id_, w_count])\n",
    "    \n",
    "    return lda.train(data,k,emmax,demmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = open(\"train.txt\",'r').read()\n",
    "d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "\n",
    "data = []\n",
    "for L in train.split(\"\\n\"):\n",
    "    if L == \"\":\n",
    "        continue\n",
    "\n",
    "    id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "    w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "    data.append([id_, w_count])\n",
    "    \n",
    "d = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10 # of classes to assume\n",
    "emmax = 100 # of maximum VB-EM iteration (default 100)\n",
    "demmax = 20 # of maximum VB-EM iteration for a document\n",
    "epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "\n",
    "# # of documents\n",
    "M = len(d)\n",
    "# # of words\n",
    "L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "\n",
    "# initialize\n",
    "phi = matrix(ones((k, L)) / L)\n",
    "alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "gammas = matrix(zeros((M, k)))\n",
    "lik = 0\n",
    "plik = lik\n",
    "phis = matrix(zeros((k, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "di, phi, alpha0 = d[1], phi, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 88)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(phi[:,di[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digamma(alpha0 + nt).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp(digamma(alpha0 + nt)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 132.19827725]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag(exp(digamma(alpha0 + nt))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 88)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(phi[:,di[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,69) and (10,10) not aligned: 69 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4bc03492117f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m#vb-estep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/3928941380/.conda/envs/py3k/lib/python3.4/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,69) and (10,10) not aligned: 69 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "di, phi, alpha0 = d[0], phi, alpha\n",
    "digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "Nd = len(di[0])\n",
    "k = len(alpha0)\n",
    "q = zeros((Nd, k))\n",
    "nt = matrix(ones((1, k)) * L / k).T\n",
    "pnt = nt\n",
    "\n",
    "\n",
    "    #vb-estep\n",
    "q = matrix( matrix(phi[:,di[0]]) * diag(exp(digamma(alpha0 + nt))[:,0]))\n",
    "q = lda.mnormalize(q , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas[1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(zeros((M, k)))[1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fpi_alpha(alpha, nt_keep):\n",
    "    # fixed point iteration for alpha / Sato p.112\n",
    "    K = alpha.shape[0]\n",
    "    M = len(nt_keep)\n",
    "\n",
    "    for k in range(K):\n",
    "        alpha_k = alpha[k, 0]\n",
    "\n",
    "        numerator = 0 ; denominator = 0\n",
    "        for m in range(M):\n",
    "            nd = nt[m].sum() # Sato p.114\n",
    "\n",
    "            # numerator\n",
    "            numerator += (digamma(nt[k,0] + alpha_k) - digamma(alpha_k)) * alpha_k\n",
    "\n",
    "            # denominator\n",
    "            denominator += digamma(nd + alpha.sum()) - digamma(alpha.sum())\n",
    "\n",
    "        alpha[k,0] = numerator / denominator\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # of documents\n",
    "M = len(d)\n",
    "# # of words\n",
    "L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "\n",
    "# initialize\n",
    "phi = matrix(ones((k, L)) / L)\n",
    "alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "gammas = matrix(zeros((M, k)))\n",
    "lik = 0\n",
    "plik = lik\n",
    "\n",
    "phis = matrix(zeros((k, L)))\n",
    "nt_keep = []\n",
    "for i in range(M):\n",
    "    gamma, q, nt = lda.vbem(d[i], phi, alpha, demmax)\n",
    "    nt_keep.append(nt)\n",
    "    gammas[i,:] = gamma.T\n",
    "    phis = lda.accum_phi(phis,q,d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnormalize(m, d=0):\n",
    "    '''\n",
    "    x = mnormalize(m, d)\n",
    "    normalizes a 2-D matrix m along the dimension d.\n",
    "    m : matrix\n",
    "    d : dimension to normalize (default 0)\n",
    "    '''\n",
    "    m = array(m)\n",
    "    v = sum(m, d)\n",
    "    if d == 0:\n",
    "        return m * matrix(diag(1.0 / v))\n",
    "    else:\n",
    "        return matrix(diag(1.0 / v)) * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "egamma = matrix(mnormalize(gammas, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nt_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nt_keep[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def lda_lik(d, phi, gammas):\n",
    "    '''\n",
    "    lik = lda_lik(d, phi, gammas)\n",
    "    returns the likelihood of d, given LDA model of (phi, gammas).\n",
    "    '''\n",
    "    egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "    lik = 0\n",
    "    M = len(d)\n",
    "    for i in range(M):\n",
    "        t = d[i]\n",
    "        lik += (matrix(t[1]).T * log(matrix(phi[:,t[0]]) * egamma[i,:].T))[0,0]\n",
    "    return lik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(matrix(t[1]) * log(matrix(phi[:,t[0]]).T * egamma[i,:].T))[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 88)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(t[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(matrix(phi[:,t[0]]).T * egamma[i,:].T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 88)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(phi[:,t[0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egamma[i,:].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "egamma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lda():\n",
    "    '''\n",
    "    Latent Dirichlet Allocation, standard model.\n",
    "    [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "    d      : data of documents\n",
    "    k      : # of classes to assume\n",
    "    emmax  : # of maximum VB-EM iteration (default 100)\n",
    "    demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(d, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation, standard model.\n",
    "        [alpha,phi] = lda.train(d,k,[emmax,demmax])\n",
    "        d      : data of documents\n",
    "        k      : # of classes to assume\n",
    "        emmax  : # of maximum VB-EM iteration (default 100)\n",
    "        demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "        '''\n",
    "        \n",
    "        # # of documents\n",
    "        M = len(d)\n",
    "        # # of words\n",
    "        L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "        \n",
    "        # initialize\n",
    "        phi = matrix(ones((k, L)) / L)\n",
    "        alpha = matrix(lda.normalize(sorted([random() for i in range(k)], reverse=True))).T\n",
    "        gammas = matrix(zeros((M, k)))\n",
    "        lik = 0\n",
    "        plik = lik\n",
    "        \n",
    "        #print ('number of documents (M)      = {0}'.format(M))\n",
    "        #print ('number of words (l)          = {0}'.format(L))\n",
    "        #print ('number of latent classes (k) = {0}'.format(k))\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            print ('iteration {0}/{1}..\\t'.format(j+1, emmax))\n",
    "            #vb-esstep\n",
    "            phis = matrix(zeros((k, L)))\n",
    "            nt_keep = []\n",
    "            for i in range(M):\n",
    "                gamma, q, nt = lda.vbem(d[i], phi, alpha, demmax)\n",
    "                nt_keep.append(nt)\n",
    "                gammas[i,:] = gamma.T\n",
    "                phis = lda.accum_phi(phis,q,d[i])\n",
    "            #vb-mstep\n",
    "            alpha = lda.fpi_alpha(alpha, nt_keep)\n",
    "                    #alpha = lda.newton_alpha(gammas)\n",
    "            phi = lda.mnormalize(phis,1)\n",
    "            #converge?\n",
    "            lik = lda.lda_lik(d, phi, gammas)\n",
    "            print ('log-likelihoood =', lik)\n",
    "            if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "                if j < 5:\n",
    "                    print\n",
    "                    return lda.train(d, k, emmax, demmax) # try again\n",
    "                    return\n",
    "                print ('converged')\n",
    "                return alpha, phi\n",
    "            plik = lik\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def vbem(di, phi, alpha0, emmax=20):\n",
    "        '''\n",
    "        [alpha,q] = vbem(d,phi,alpha0,[emmax])\n",
    "        calculates a document and words posterior for a document d.\n",
    "        alpha  : Dirichlet posterior for a document d\n",
    "        q      : (L * K) matrix of word posterior over latent classes\n",
    "        di      : document data / here, only one sentence\n",
    "        phi   : \n",
    "        alpha0 : Dirichlet prior of alpha\n",
    "        emmax  : maximum # of VB-EM iteration.\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        Nd = len(di[0])\n",
    "        k = len(alpha0)\n",
    "        q = zeros((Nd, k))\n",
    "        nt = matrix(ones((1, k)) * Nd / k).T\n",
    "        pnt = nt\n",
    "\n",
    "        for j in range(emmax):\n",
    "            #vb-estep\n",
    "            q = matrix( matrix(phi[:,di[0]]).T * diag(exp(digamma(alpha0 + nt))[:,0]))\n",
    "            q = lda.mnormalize(q , 1)\n",
    "                    # q(z_d,i =k)\n",
    "                    # alpha0 + ntでよくわからないけど、alphaをアップデートしてるからe-step?\n",
    "                    # better to look at original C code\n",
    "                       #  ap[k]というのが、式のexpの分子部分相当\n",
    "                        #  それにphiをかけて最後に正規化している\n",
    "            #vb-mstep\n",
    "            nt =  q.T * matrix(di[1]).T\n",
    "                    # nt:  probably expectation part in Sato Equation(3.89), same as bottom in Sato p.75\n",
    "                    #  Sato p.75にあるように本来ならば文章中の一語ずつチェックするが、データセットには\n",
    "                    #  各単語ごとの出現回数しかないので、単語が登場するたびに足すのではなく、単語の出現回数×qをして\n",
    "                    #  足しあわせている\n",
    "                    # better to look at original C code\n",
    "                \n",
    "            #converge?\n",
    "            if j > 1 and lda.converged(nt, pnt, 1.0e-2):\n",
    "                break\n",
    "            pnt = nt.copy()\n",
    "            \n",
    "        alpha = alpha0 + nt # corresponds to Sato Eq (3.89) / dの全てのkに対して一度にしている\n",
    "                                        # nt:  probably expectation part in Sato Equation(3.89)\n",
    "        return alpha, q, nt\n",
    "    \n",
    "    @staticmethod\n",
    "    def fpi_alpha(alpha, nt_keep):\n",
    "        # fixed point iteration for alpha / Sato p.112\n",
    "        K = alpha.shape[0]\n",
    "        M = len(nt_keep)\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        for k in range(K):\n",
    "            alpha_k = alpha[k, 0]\n",
    "\n",
    "            numerator = 0 ; denominator = 0\n",
    "            for m in range(M):\n",
    "                nt = nt_keep[m]\n",
    "                nd = nt.sum() # Sato p.114\n",
    "\n",
    "                # numerator\n",
    "                numerator += (digamma(nt[k,0] + alpha_k) - digamma(alpha_k)) * alpha_k\n",
    "\n",
    "                # denominator\n",
    "                denominator += digamma(nd + alpha.sum()) - digamma(alpha.sum())\n",
    "\n",
    "            alpha[k,0] = numerator / denominator\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    @staticmethod\n",
    "    def accum_phi(phis, q, t):\n",
    "        '''\n",
    "        phis = accum_phi(phis,q,t)\n",
    "        accumulates word posteriors to latent classes.\n",
    "        phis : (V * K) matrix of summand\n",
    "        q     : (L * K) matrix of word posteriors\n",
    "        t     : document of struct array\n",
    "        '''\n",
    "        phis[: , t[0]] +=  q.T  * matrix(diag(t[1])).T\n",
    "           # phis[t[0],:].shape \n",
    "                # this matrix is (number of unique words in document no1) ¥times  (number of class (category) )\n",
    "                # t[0] shows word id_\n",
    "        return phis\n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_lik(d, phi, gammas):\n",
    "        '''\n",
    "        lik = lda_lik(d, phi, gammas)\n",
    "        returns the likelihood of d, given LDA model of (phi, gammas).\n",
    "        '''\n",
    "        egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "        lik = 0\n",
    "        M = len(d)\n",
    "        for i in range(M):\n",
    "            t = d[i]\n",
    "            lik += (matrix(t[1]) * log(matrix(phi[:,t[0]]).T * egamma[i,:].T))[0,0]\n",
    "        return lik\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(v):\n",
    "        return v / sum(v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mnormalize(m, d=0):\n",
    "        '''\n",
    "        x = mnormalize(m, d)\n",
    "        normalizes a 2-D matrix m along the dimension d.\n",
    "        m : matrix\n",
    "        d : dimension to normalize (default 0)\n",
    "        '''\n",
    "        m = array(m)\n",
    "        v = sum(m, d)\n",
    "        if d == 0:\n",
    "            return m * matrix(diag(1.0 / v))\n",
    "        else:\n",
    "            return matrix(diag(1.0 / v)) * m\n",
    "        \n",
    "    @staticmethod\n",
    "    def converged(u, udash, threshold=1.0e-3):\n",
    "        '''\n",
    "        converged(u,udash,threshold)\n",
    "        Returns 1 if u and udash are not different by the ratio threshold\n",
    "        '''\n",
    "        return norm(u - udash) / norm(u) < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha, phi = main()\n",
    "    \n",
    "    res_b = pd.DataFrame(phi)\n",
    "    for i in range(phi.shape[0]):\n",
    "        res_temp = res_b.ix[i, :].sort_values(ascending=False)[:10]\n",
    "        res_temp.index += 1\n",
    "        print(\"Topic: \", i)\n",
    "        print(res_temp)\n",
    "        print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
