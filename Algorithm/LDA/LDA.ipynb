{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://satomacoto.blogspot.jp/2009/12/pythonlda.html   \n",
    "Code in the reference is modified for Python 3  \n",
    "\n",
    "LDA ([Blei et al.](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Figure 1)  \n",
    "Variational Bayes EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,getopt\n",
    "from numpy import array,matrix,diag\n",
    "from scipy import sum,log,exp,mean,dot,ones,zeros\n",
    "from scipy.special import polygamma\n",
    "from scipy.linalg import norm\n",
    "from random import random\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/3928941380/Downloads/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # set parameters\n",
    "    k = 10 # of classes to assume\n",
    "    emmax = 100 # of maximum VB-EM iteration (default 100)\n",
    "    demmax = 20 # of maximum VB-EM iteration for a document\n",
    "    epsilon = 0.0001 # A threshold to determine the whole convergence of the estimation\n",
    "    \n",
    "    # Train\n",
    "    train = open(\"train.txt\",'r').read()\n",
    "    alpha,beta = ldamain(train, k, emmax, demmax, epsilon)\n",
    "    \n",
    "    # Write\n",
    "    writer = open('output-alpha.txt','w')\n",
    "    writer.write(str(alpha.tolist()))\n",
    "    writer.close() \n",
    "    \n",
    "    writer = open('output-beta.txt','w')\n",
    "    writer.write(str(beta.tolist()))\n",
    "    writer.close()\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ldamain(train, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "    d = [ zip(*[ [int(x) for x in w.split(':')] for w in L.split()]) for L in train.split('\\n') if L ]\n",
    "    \n",
    "    data = []\n",
    "    for L in train.split(\"\\n\"):\n",
    "        if L == \"\":\n",
    "            continue\n",
    "\n",
    "        id_ = [int(w.split(\":\")[0]) for w in L.split(\" \")]\n",
    "        w_count = [int(w.split(\":\")[1]) for w in L.split(\" \")]\n",
    "\n",
    "        data.append([id_, w_count])\n",
    "    \n",
    "    return lda.train(data,k,emmax,demmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originalではtupleになっていたものを、listで書いた:\n",
    "\n",
    "```\n",
    ">>> d[0]\n",
    "[(10, 37, 40, 43, 62, 72, 75, 102, 111, 115, 131, 164, 208, 281, 315, 321, 331, 355, 368, 377, 379, 392, 416, 419, 434, 435, 477, 487, 499, 501, 586, 594, 598, 618, 621, 637, 677, 692, 702, 799, 808, 809, 828, 896, 901, 908, 941, 988, 1035, 1065, 1083, 1085, 1112, 1124, 1141, 1170, 1171, 1175, 1178, 1188, 1197, 1198, 1219, 1270, 1285, 1292, 1293, 1308, 1311), (7, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 1, 1, 3, 1, 3, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 4, 1, 2, 1, 1, 1, 4, 1, 2, 10, 1, 1, 2, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 12, 1, 1, 2, 5, 1, 1, 1, 4, 1, 1, 3, 2)]\n",
    "\n",
    ">>> d[0][0]\n",
    "(10, 37, 40, 43, 62, 72, 75, 102, 111, 115, 131, 164, 208, 281, 315, 321, 331, 355, 368, 377, 379, 392, 416, 419, 434, 435, 477, 487, 499, 501, 586, 594, 598, 618, 621, 637, 677, 692, 702, 799, 808, 809, 828, 896, 901, 908, 941, 988, 1035, 1065, 1083, 1085, 1112, 1124, 1141, 1170, 1171, 1175, 1178, 1188, 1197, 1198, 1219, 1270, 1285, 1292, 1293, 1308, 1311)\n",
    "\n",
    ">>> train.split(\"\\n\")[0]\n",
    "'10:7 37:1 40:1 43:1 62:3 72:1 75:1 102:1 111:2 115:1 131:1 164:1 208:1 281:1 315:3 321:1 331:3 355:1 368:2 377:1 379:1 392:1 416:2 419:1 434:1 435:2 477:1 487:2 499:1 501:1 586:4 594:1 598:2 618:1 621:1 637:1 677:4 692:1 702:2 799:10 808:1 809:1 828:2 896:1 901:1 908:1 941:1 988:1 1035:1 1065:5 1083:1 1085:1 1112:1 1124:1 1141:1 1170:1 1171:12 1175:1 1178:1 1188:2 1197:5 1198:1 1219:1 1270:1 1285:4 1292:1 1293:1 1308:3 1311:2'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class lda():\n",
    "    '''\n",
    "    Latent Dirichlet Allocation, standard model.\n",
    "    [alpha,beta] = lda.train(d,k,[emmax,demmax])\n",
    "    d      : data of documents\n",
    "    k      : # of classes to assume\n",
    "    emmax  : # of maximum VB-EM iteration (default 100)\n",
    "    demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(d, k, emmax=100, demmax=20, epsilon=1.0e-4):\n",
    "        '''\n",
    "        Latent Dirichlet Allocation, standard model.\n",
    "        [alpha,beta] = lda.train(d,k,[emmax,demmax])\n",
    "        d      : data of documents\n",
    "        k      : # of classes to assume\n",
    "        emmax  : # of maximum VB-EM iteration (default 100)\n",
    "        demmax : # of maximum VB-EM iteration for a document (default 20)\n",
    "        '''\n",
    "        \n",
    "        # # of documents\n",
    "        n = len(d)\n",
    "        # # of words\n",
    "        L = max(map(lambda x: max(x[0]), d)) + 1\n",
    "        \n",
    "        # initialize\n",
    "        beta = ones((L, k)) / L\n",
    "        alpha = lda.normalize(sorted([random() for i in range(k)], reverse=True))\n",
    "        gammas = zeros((n, k))\n",
    "        lik = 0\n",
    "        plik = lik\n",
    "        \n",
    "        print ('number of documents (n)      = {0}'.format(n))\n",
    "        print ('number of words (l)          = {0}'.format(L))\n",
    "        print ('number of latent classes (k) = {0}'.format(k))\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            print ('iteration {0}/{1}..\\t'.format(j+1, emmax))\n",
    "            #vb-esstep\n",
    "            betas = zeros((L, k))\n",
    "            for i in range(n):\n",
    "                gamma,q = lda.vbem(d[i], beta, alpha, demmax)\n",
    "                gammas[i,:] = gamma\n",
    "                betas = lda.accum_beta(betas,q,d[i])\n",
    "            #vb-mstep\n",
    "            alpha = lda.newton_alpha(gammas)\n",
    "            beta = lda.mnormalize(betas,0)\n",
    "            #converge?\n",
    "            lik = lda.lda_lik(d, beta, gammas)\n",
    "            print ('log-likelihoood =', lik)\n",
    "            if j > 1 and abs((lik - plik) / lik) < epsilon:\n",
    "                if j < 5:\n",
    "                    print\n",
    "                    return lda.train(d, k, emmax, demmax) # try again\n",
    "                    return\n",
    "                print ('converged')\n",
    "                return alpha, beta\n",
    "            plik = lik\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    def vbem(d, beta, alpha0, emmax=20):\n",
    "        '''\n",
    "        [alpha,q] = vbem(d,beta,alpha0,[emmax])\n",
    "        calculates a document and words posterior for a document d.\n",
    "        alpha  : Dirichlet posterior for a document d\n",
    "        q      : (L * K) matrix of word posterior over latent classes\n",
    "        d      : document data\n",
    "        beta   : \n",
    "        alpha0 : Dirichlet prior of alpha\n",
    "        emmax  : maximum # of VB-EM iteration.\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0,x)\n",
    "\n",
    "        L = len(d[0])\n",
    "        k = len(alpha0)\n",
    "        q = zeros((L, k))\n",
    "        nt = ones((1, k)) * L / k\n",
    "        pnt = nt\n",
    "        \n",
    "        for j in range(emmax):\n",
    "            #vb-estep\n",
    "            q = lda.mnormalize(matrix(beta[d[0],:]) * matrix(diag(exp(digamma(alpha0 + nt))[0])), 1)\n",
    "            #vb-mstep\n",
    "            nt = matrix(d[1]) * q\n",
    "            #converge?\n",
    "            if j > 1 and lda.converged(nt, pnt, 1.0e-2):\n",
    "                break\n",
    "            pnt = nt.copy()\n",
    "        alpha = alpha0 + nt\n",
    "        return alpha, q\n",
    "\n",
    "    @staticmethod\n",
    "    def accum_beta(betas, q, t):\n",
    "        '''\n",
    "        betas = accum_beta(betas,q,t)\n",
    "        accumulates word posteriors to latent classes.\n",
    "        betas : (V * K) matrix of summand\n",
    "        q     : (L * K) matrix of word posteriors\n",
    "        t     : document of struct array\n",
    "        '''\n",
    "        betas[t[0],:] += matrix(diag(t[1])) * q        \n",
    "        return betas\n",
    "    \n",
    "    @staticmethod\n",
    "    def lda_lik(d, beta, gammas):\n",
    "        '''\n",
    "        lik = lda_lik(d, beta, gammas)\n",
    "        returns the likelihood of d, given LDA model of (beta, gammas).\n",
    "        '''\n",
    "        egamma = matrix(lda.mnormalize(gammas, 1))\n",
    "        lik = 0\n",
    "        n = len(d)\n",
    "        for i in range(n):\n",
    "            t = d[i]\n",
    "            lik += (matrix(t[1]) * log(matrix(beta[t[0],:]) * egamma[i,:].T))[0,0]\n",
    "        return lik\n",
    "\n",
    "    @staticmethod\n",
    "    def newton_alpha(gammas,maxiter=20,ini_alpha=[]):\n",
    "        '''\n",
    "        alpha = newton_alpha (gammas,[maxiter])\n",
    "        Newton-Raphson iteration of LDA Dirichlet prior.\n",
    "        gammas  : matrix of Dirichlet posteriors (M * k)\n",
    "        maxiter : # of maximum iteration of Newton-Raphson\n",
    "        '''\n",
    "        digamma = lambda x: polygamma(0, x)\n",
    "        trigamma = lambda x: polygamma(1, x)\n",
    "\n",
    "        M,K = gammas.shape\n",
    "        \n",
    "        if not M > 1:\n",
    "            return gammas[1,:]\n",
    "        if not len(ini_alpha) > 0:\n",
    "            ini_alpha = mean(gammas, 0) / K\n",
    "        \n",
    "        L = 0\n",
    "        g = zeros((1,K))\n",
    "        pg = sum(digamma(gammas), 0) - sum(digamma(sum(gammas, 1)))\n",
    "        alpha = ini_alpha.copy()\n",
    "        palpha = zeros((1, K))\n",
    "        \n",
    "        for t in range(maxiter):\n",
    "            L += 1\n",
    "            alpha0 = sum(alpha)\n",
    "            g = M * (digamma(alpha0) - digamma(alpha)) + pg\n",
    "            h = -1.0 / trigamma(alpha)\n",
    "            hgz = dot(h,g) / (1.0 / trigamma(alpha0) + sum(h))\n",
    "\n",
    "            for i in range(K):\n",
    "                alpha[i] = alpha[i] - h[i] * (g[i] - hgz) / M\n",
    "                if alpha[i] < 0:\n",
    "                    return lda.newton_alpha(gammas, maxiter, ini_alpha / 10.0)\n",
    "            \n",
    "            #converge?\n",
    "            if L > 1 and lda.converged(alpha, palpha, 1.0e-4):\n",
    "                break\n",
    "            \n",
    "            palpha = alpha.copy()\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(v):\n",
    "        return v / sum(v)\n",
    "    \n",
    "    @staticmethod\n",
    "    def mnormalize(m, d=0):\n",
    "        '''\n",
    "        x = mnormalize(m, d)\n",
    "        normalizes a 2-D matrix m along the dimension d.\n",
    "        m : matrix\n",
    "        d : dimension to normalize (default 0)\n",
    "        '''\n",
    "        m = array(m)\n",
    "        v = sum(m, d)\n",
    "        if d == 0:\n",
    "            return m * matrix(diag(1.0 / v))\n",
    "        else:\n",
    "            return matrix(diag(1.0 / v)) * m\n",
    "        \n",
    "    @staticmethod\n",
    "    def converged(u, udash, threshold=1.0e-3):\n",
    "        '''\n",
    "        converged(u,udash,threshold)\n",
    "        Returns 1 if u and udash are not different by the ratio threshold\n",
    "        '''\n",
    "        return norm(u - udash) / norm(u) < threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alphaは長さがトピック数のリスト。betaはトピックの数×語の数の二次元リスト\n",
    "\n",
    "alpha:  \n",
    "```\n",
    "array([ 0.04939266,  0.08121416,  0.06315083,  0.07805509,  0.06456953,\n",
    "        0.05401515,  0.04950355,  0.04082875,  0.03303216,  0.04126227])\n",
    "```\n",
    "\n",
    "beta.shape:\n",
    "```\n",
    "(1325,10)\n",
    "```\n",
    "\n",
    "beta[1]:\n",
    "```\n",
    "matrix([[  3.13629002e-03,   2.14675287e-03,   9.11276558e-04,\n",
    "           1.25669929e-03,   8.50530562e-17,   2.68656272e-03,\n",
    "           4.44369171e-03,   1.27595937e-03,   1.70240226e-03,\n",
    "           2.38235775e-03]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha, beta = main()\n",
    "    \n",
    "    res_b = pd.DataFrame(beta)\n",
    "    for i in range(beta.shape[1]):\n",
    "        res_temp = res_b.ix[:, i].sort_values(ascending=False)[:10]\n",
    "        res_temp.index += 1\n",
    "        print(\"Topic: \", i)\n",
    "        print(res_temp)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "1172    0.084061\n",
      "800     0.047266\n",
      "11      0.041168\n",
      "73      0.030901\n",
      "587     0.027400\n",
      "1198    0.024197\n",
      "1163    0.021027\n",
      "488     0.018967\n",
      "1288    0.017978\n",
      "99      0.013563\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  1\n",
      "1172    0.112207\n",
      "800     0.052600\n",
      "11      0.039496\n",
      "587     0.034232\n",
      "73      0.023060\n",
      "1198    0.022889\n",
      "643     0.021550\n",
      "478     0.020423\n",
      "863     0.017516\n",
      "488     0.014548\n",
      "Name: 1, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  2\n",
      "1172    0.096230\n",
      "800     0.067011\n",
      "73      0.033424\n",
      "11      0.027376\n",
      "587     0.025825\n",
      "643     0.021830\n",
      "488     0.020784\n",
      "1198    0.019900\n",
      "478     0.012800\n",
      "99      0.012666\n",
      "Name: 2, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  3\n",
      "1172    0.104749\n",
      "800     0.056938\n",
      "73      0.032993\n",
      "11      0.024905\n",
      "643     0.024700\n",
      "1198    0.024061\n",
      "587     0.019876\n",
      "488     0.019068\n",
      "666     0.016262\n",
      "158     0.014835\n",
      "Name: 3, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  4\n",
      "1172    0.085544\n",
      "800     0.064253\n",
      "73      0.029557\n",
      "11      0.028619\n",
      "1198    0.025533\n",
      "643     0.021189\n",
      "587     0.020689\n",
      "488     0.018634\n",
      "99      0.017819\n",
      "1171    0.014782\n",
      "Name: 4, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  5\n",
      "1172    0.101309\n",
      "800     0.066887\n",
      "73      0.028382\n",
      "11      0.019658\n",
      "643     0.019643\n",
      "488     0.019453\n",
      "1198    0.018898\n",
      "891     0.017905\n",
      "702     0.016629\n",
      "112     0.015086\n",
      "Name: 5, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  6\n",
      "1172    0.090549\n",
      "800     0.071266\n",
      "73      0.031043\n",
      "1213    0.022827\n",
      "802     0.021357\n",
      "1002    0.020538\n",
      "587     0.019743\n",
      "112     0.019499\n",
      "1198    0.019321\n",
      "11      0.018841\n",
      "Name: 6, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  7\n",
      "1172    0.084907\n",
      "800     0.062620\n",
      "1198    0.033226\n",
      "73      0.031992\n",
      "11      0.021548\n",
      "587     0.018458\n",
      "643     0.017988\n",
      "99      0.016325\n",
      "49      0.013378\n",
      "133     0.013296\n",
      "Name: 7, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  8\n",
      "1172    0.116543\n",
      "800     0.084352\n",
      "11      0.035933\n",
      "73      0.029953\n",
      "587     0.028650\n",
      "1198    0.020657\n",
      "1019    0.014667\n",
      "643     0.013905\n",
      "9       0.012429\n",
      "478     0.010438\n",
      "Name: 8, dtype: float64\n",
      "\n",
      "\n",
      "Topic:  9\n",
      "1172    0.105248\n",
      "800     0.040352\n",
      "643     0.036354\n",
      "1198    0.031349\n",
      "587     0.028380\n",
      "73      0.025462\n",
      "1171    0.020964\n",
      "488     0.020107\n",
      "11      0.017673\n",
      "647     0.015192\n",
      "Name: 9, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res_b = pd.DataFrame(beta)\n",
    "\n",
    "for i in range(beta.shape[1]):\n",
    "    res_temp = res_b.ix[:, i].sort_values(ascending=False)[:10]\n",
    "    res_temp.index += 1\n",
    "    print(\"Topic: \", i)\n",
    "    print(res_temp)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
